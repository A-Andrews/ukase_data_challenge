{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for feature importance\n",
    "\n",
    "The idea of this pipeline is to select significant features from a list of features. Presented with a series of rules for feature evaluation the pipeline will run through all of these and record the performance of the models and the importance of features in determining the decisions of the models. Then we should be able to evaluate that data to decide what to include in our model for general prediction. It could be that from each group of features one is particularly important it could also be that there is interaction between groups that makes this significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = path.join(\"..\", \"data\", \"zoonosis_dataset_full_full.csv\")\n",
    "target_column = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_ml(df, target_column=None, one_hot_encode=True):\n",
    "    \"\"\"\n",
    "    Prepare a pandas DataFrame for machine learning algorithms.\n",
    "    - Normalizes numerical features\n",
    "    - Optionally one-hot encodes categorical features\n",
    "    - Optionally separates target variable\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame to prepare\n",
    "    target_column : str, optional\n",
    "        Name of the target column to separate\n",
    "    one_hot_encode : bool, optional\n",
    "        Whether to one-hot encode categorical features\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_processed: pandas.DataFrame\n",
    "        The processed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Separate target if specified\n",
    "    y = None\n",
    "    if target_column and target_column in df_processed.columns:\n",
    "        y = df_processed[target_column].replace({\"nz\": 0, \"hzoon\": 1})\n",
    "        df_processed = df_processed.drop(columns=[target_column])\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df_processed.select_dtypes(\n",
    "        include=[\"int64\", \"float64\"]\n",
    "    ).columns.tolist()\n",
    "    categorical_cols = df_processed.select_dtypes(\n",
    "        include=[\"object\", \"category\", \"bool\"]\n",
    "    ).columns.tolist()\n",
    "\n",
    "    # Handle missing values\n",
    "    df_processed[numerical_cols] = df_processed[numerical_cols].fillna(\n",
    "        df_processed[numerical_cols].median()\n",
    "    )\n",
    "    for col in categorical_cols:\n",
    "        df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])\n",
    "\n",
    "    # Normalize numerical features\n",
    "    if numerical_cols:\n",
    "        scaler = StandardScaler()\n",
    "        df_processed[numerical_cols] = scaler.fit_transform(\n",
    "            df_processed[numerical_cols]\n",
    "        )\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    if categorical_cols and one_hot_encode:\n",
    "        df_processed = pd.get_dummies(\n",
    "            df_processed, columns=categorical_cols, drop_first=False\n",
    "        )\n",
    "\n",
    "    # If we have a target column, add it back to the processed dataframe\n",
    "    if target_column and y is not None:\n",
    "        df_processed[target_column] = y\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sx/g9qrr1y94z1gtsqjsbkjw2m40000gs/T/ipykernel_27588/3979001912.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = df_processed[target_column].replace({\"nz\": 0, \"hzoon\": 1})\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "processed_data = prepare_dataframe_for_ml(\n",
    "    data, target_column=target_column\n",
    ")\n",
    "y = processed_data[\"label\"]\n",
    "X = processed_data.drop(columns=[\"label\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # (\"XGBoost\", XGBClassifier(enable_categorical=True)),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    # (\"Ridge Classifier\", RidgeClassifier()),\n",
    "    # (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    # (\"Support Vector Classification\", SVC()),\n",
    "    # (\"LightGBM\", LGBMClassifier()),\n",
    "    # (\"KNN\", KNeighborsClassifier(5, weights=\"uniform\")),\n",
    "    # (\"Naive Bayes\", GaussianNB()),\n",
    "    # (\"Neural Network\", MLPClassifier()),\n",
    "    # (\"Quadratic Discriminant Analysis\", QuadraticDiscriminantAnalysis()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, X):\n",
    "    try:\n",
    "        mdi_importances = pd.Series(\n",
    "            model.feature_importances_, index=X.columns\n",
    "        ).sort_values(ascending=True)\n",
    "        return mdi_importances\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_permutation_importance(model, X, y):\n",
    "    result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "    # Create a Series with feature names and their mean importances\n",
    "    importances = pd.Series(result.importances_mean, index=X.columns)\n",
    "    # Sort importances from most to least important\n",
    "    sorted_importances = importances.sort_values(ascending=False)\n",
    "    return sorted_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_all_models(models, X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        average_precision = average_precision_score(y_test, y_pred)\n",
    "        feature_importance = get_feature_importance(model, X_test)\n",
    "        permutation_importance = get_permutation_importance(model, X_test, y_test)\n",
    "        most_important_permutation = permutation_importance.index[0]\n",
    "        try:\n",
    "            most_important_feature = feature_importance.index[-1]\n",
    "        except AttributeError:\n",
    "            most_important_feature = None\n",
    "        results[name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"average_precision\": average_precision,\n",
    "            \"feature_importance\": feature_importance,\n",
    "            \"most_important_feature\": most_important_feature,\n",
    "            \"permutation_importance\": permutation_importance,\n",
    "            \"most_important_permutation\": most_important_permutation,\n",
    "            \"columns\": \",\".join(X_test.columns.to_list()),\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, filename):\n",
    "    results_to_be_saved = pd.DataFrame.from_dict(data=results, orient=\"index\")\n",
    "    results_to_be_saved.to_csv(path.join(\"..\", \"model_comparison_data\", filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Level Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_list_to_dataframe(results):\n",
    "    results_df = pd.DataFrame.from_dict(data=results, orient=\"index\")\n",
    "    # data_frame = data_frame.append(results_df, ignore_index=True)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_test_train_sets(X, y, columns_to_include, test_size=0.2, random_state=42):\n",
    "    X_dropped = X[columns_to_include]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_dropped, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(rules, models, X, y):\n",
    "    rule_results = {}\n",
    "    for rule in rules:\n",
    "        rule_name = rule\n",
    "        columns_to_include = rules[rule]\n",
    "        X_train, X_test, y_train, y_test = get_new_test_train_sets(\n",
    "            X, y, columns_to_include\n",
    "        )\n",
    "        results = get_results_all_models(models, X_train, X_test, y_train, y_test)\n",
    "        rule_results[rule_name] = results\n",
    "    return rule_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns_from_rule(rule, columns):\n",
    "    rule_columns = [c for c in columns if re.fullmatch(rule, c)]\n",
    "    return rule_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules_dict(rules, columns):\n",
    "    rules_dict = {}\n",
    "    for rule in rules:\n",
    "        rule_name = rule\n",
    "        rule_columns = extract_columns_from_rule(rule, columns)\n",
    "        rules_dict[rule_name] = rule_columns\n",
    "    return rules_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = [r\"HA\", r\"NA\", r\"M1\", r\"NS1\", r\"NP\", r\"PA\", r\"PB1\", r\"PB2\"]\n",
    "prefix = [\n",
    "    r\"\\b([CTGA]){2}_\",\n",
    "    # r\"\\b([CTGA]){3}_\",\n",
    "    # r\"\\b([CTGA]){4}_\",\n",
    "    # r\"\\b([CTGA]){5}_\",\n",
    "    # r\"\\b([CTGA]){6}_\",\n",
    "    r\"\\bDPC_.*_\",\n",
    "    r\"\\bCTDC_.*_\",\n",
    "    r\"\\bCTDD_.*_\",\n",
    "    r\"\\bCTDT_.*_\",\n",
    "    r\"\\bCTriad_.*_\",\n",
    "    r\"\\bPAAC_.*_\",\n",
    "]\n",
    "# prefix = [\n",
    "#     r\"\\w+(Bias)_\",\n",
    "# ]\n",
    "rules = [p + g for p in prefix for g in genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_dict = get_rules_dict(rules, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_rule_results = run_pipeline(rules_dict, models, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(nested_dict):\n",
    "    res = {}\n",
    "    if isinstance(nested_dict, dict):\n",
    "        for k in nested_dict:\n",
    "            flattened_dict = flatten_dict(nested_dict[k])\n",
    "            for key, val in flattened_dict.items():\n",
    "                key = list(key)\n",
    "                key.insert(0, k)\n",
    "                res[tuple(key)] = val\n",
    "    else:\n",
    "        res[()] = nested_dict\n",
    "    return res\n",
    "\n",
    "\n",
    "def nested_dict_to_df(values_dict):\n",
    "    flat_dict = flatten_dict(values_dict)\n",
    "    df = pd.DataFrame.from_dict(flat_dict, orient=\"index\")\n",
    "    df.index = pd.MultiIndex.from_tuples(df.index)\n",
    "    df = df.unstack(level=-1)\n",
    "    df.columns = df.columns.map(\"{0[1]}\".format)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = nested_dict_to_df(all_rule_results)\n",
    "output_dataframe.index.set_names([\"Rules\", \"Models\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe.to_csv(path.join(\"..\", \"model_comparison_data\", \"gene_division_bias_results_random_forest.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe.to_excel(path.join(\"..\", \"model_comparison_data\", \"gene_division_bias_results_random_forest.xlsx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
